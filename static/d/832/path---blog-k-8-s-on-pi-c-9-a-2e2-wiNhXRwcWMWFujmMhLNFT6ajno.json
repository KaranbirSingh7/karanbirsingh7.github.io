{"data":{"markdownRemark":{"html":"<img src=\"https://storage.googleapis.com/ansible-assets/k8s-rpi-cover.png\" alt=\"Hardware\" title=\"Hardware\" width=\"100%\">\n<p>(Gopher image by <a href=\"https://github.com/ashleymcnamara/gophers\">Ashley McNamara</a>)</p>\n<p>GitHub project: <a href=\"https://github.com/ljfranklin/k8s-pi\">https://github.com/ljfranklin/k8s-pi</a></p>\n<p>X-Posted to <a href=\"https://medium.com/@lylejfranklin/a-production-ish-kubernetes-cluster-on-raspberry-pi-d451dc6e1b17\">Medium</a></p>\n<h2 id=\"table-of-contents\"><a href=\"#table-of-contents\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Table of contents:</h2>\n<ul>\n<li><a href=\"#lets-build\">Let's Build</a></li>\n<li><a href=\"#why-though\">Why though?</a></li>\n<li><a href=\"#hardware\">Hardware</a></li>\n<li><a href=\"#networking\">Networking</a></li>\n<li><a href=\"#storage\">Storage</a></li>\n<li><a href=\"#installing-software-with-helm\">Installing software with Helm</a></li>\n<li><a href=\"#initial-setup\">Initial Setup</a></li>\n<li><a href=\"#optional-access-k8s-dashboard\">Optional: Access K8S Dashboard</a></li>\n<li><a href=\"#optional-connect-to-cluster-with-vpn\">Optional: Connect to cluster with VPN</a></li>\n<li><a href=\"#optional-backuprestore\">Optional: Backup/Restore</a></li>\n<li><a href=\"#optional-adding-your-own-ansible-tasks\">Optional: Adding your own Ansible tasks</a></li>\n<li><a href=\"#optional-building-arm-images\">Optional: Building ARM images</a></li>\n<li><a href=\"#open-issues\">Open Issues</a></li>\n<li><a href=\"#future-work\">Future work</a></li>\n<li><a href=\"#finished\">Finished!</a></li>\n</ul>\n<h2 id=\"lets-build\"><a href=\"#lets-build\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Let's Build</h2>\n<p>This guide shows how to build a \"production-ish\" Kubernetes (k8s) cluster on Raspberry Pi hardware.\nThere are many existing guides and tools available telling you how to deploy a \"production-grade\" k8s cluster, but\nproduction-grade feels like a stretch when talking about a small stack of $30 single board computers.\nSo this guide shoots for a production-ish k8s cluster, meaning you can interact with it as you would a production k8s cluster\neven if the hardware would have problems handling production workloads.</p>\n<p>More specifically we want to support the following features:</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#dynamic\">Dynamic Persistent Volumes</a> to add persistent data to pods</li>\n<li>Externally accessible URLs</li>\n<li>Load balancing of requests across multiple containers</li>\n<li>Automatic backup and restore workflow</li>\n<li>Easy installation of services via <a href=\"https://helm.sh/\">Helm</a></li>\n<li>No out-of-band configuration of router or DNS records after initial setup</li>\n<li>Auto-renewing TLS certificates from <a href=\"https://letsencrypt.org/\">Let's Encrypt</a></li>\n<li>VPN access to cluster for debugging</li>\n</ul>\n<p>Most Kubernetes on Raspberry Pi guides show only a minimal installation.\nIt works but requires manually adjusting router settings and DNS records each time you add a new service.\nYou also have to use <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\">hostPath</a> volumes for persistent data,\nforcing pods to be locked to a specific worker node.\nOne of the main benefits of Kubernetes is that it abstracts away the underlying infrastructure so you don't need to know whether you're running on\nGKE or AWS or bare metal, but these limitations mean we're not realizing that benefit.\nThis guide removes those limitations so the interacting with Kubernetes on Raspberry Pi <em>feels like</em> interacting with a production cluster.</p>\n<h2 id=\"why-though\"><a href=\"#why-though\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why though?</h2>\n<p>You might already be asking the following question: isn't this overkill for a Raspberry Pi setup?\nMy answer: yes, absolutely.\nThis setup has a ton of moving pieces, and combined with the fast-moving k8s ecosystem and limited hardware capabilities it can be difficult to debug\nwhen something goes wrong.\nSimply installing a Debian package directly onto the Raspberry Pi is a simpler and probably more stable setup than installing that app on k8s on underpowered hardware.\nThe primary goal of this setup is to learn more about Kubernetes and bare metal infrastructure (deployment/networking/storage) with some first-hand experimentation.\nPlus this setup is pretty delightful to use once you get it working.</p>\n<h2 id=\"hardware\"><a href=\"#hardware\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hardware</h2>\n<p>(see image above)</p>\n<p>Much of the hardware selection was taken from Scott Hanselman's excellent\n<a href=\"https://www.hanselman.com/blog/HowToBuildAKubernetesClusterWithARMRaspberryPiThenRunNETCoreOnOpenFaas.aspx\">How to Build a Kubernetes Cluster with ARM Raspberry Pi</a> guide.\nCheck out his guide for some extra rationale for why he chose each part.\nTLDR: buy tiny hardware that looks cute next to a stack of tiny Raspberry Pis.</p>\n<p>Parts:</p>\n<table>\n<thead>\n<tr>\n<th>Price</th>\n<th>Count</th>\n<th>Part</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$210</td>\n<td>6x</td>\n<td><a href=\"https://www.pishop.us/product/raspberry-pi-3-model-b-plus/?src=raspberrypi\">Raspberry Pi 3 B+</a></td>\n</tr>\n<tr>\n<td>-</td>\n<td>-</td>\n<td>We'll use 1 master node and 5 worker nodes but you can adjust the number of worker nodes</td>\n</tr>\n<tr>\n<td>$72</td>\n<td>6x</td>\n<td><a href=\"http://amzn.to/2iEPjGg\">32 GB MicroSD cards</a></td>\n</tr>\n<tr>\n<td>$18</td>\n<td>12x</td>\n<td><a href=\"http://amzn.to/2zUxVRX\">1 foot flat ethernet cables</a> (buy two packs, 12 cables total)</td>\n</tr>\n<tr>\n<td>$32</td>\n<td>1x</td>\n<td><a href=\"http://amzn.to/2zV6reM\">Anker PowerPort 6 Port USB Charging Hub</a></td>\n</tr>\n<tr>\n<td>$40</td>\n<td>1x</td>\n<td><a href=\"http://amzn.to/2i9n0M5\">stacking Raspberry Pi case</a></td>\n</tr>\n<tr>\n<td>$40</td>\n<td>1x</td>\n<td><a href=\"http://amzn.to/2gNzLzi\">USB-powered 8 port switch</a></td>\n</tr>\n<tr>\n<td>$10</td>\n<td>1x</td>\n<td><a href=\"https://www.amazon.com/UGREEN-Reader-Memory-Windows-Simultaneously/dp/B01EFPX9XA\">SD card reader</a> (if you don't already have one)</td>\n</tr>\n<tr>\n<td>$14</td>\n<td>1x</td>\n<td><a href=\"https://www.amazon.com/gp/product/B00LFVITLK/\">32GB USB</a></td>\n</tr>\n<tr>\n<td>$139</td>\n<td>1x</td>\n<td>(optional) <a href=\"https://www.ubnt.com/unifi-routing/usg/\">Unifi Security Gateway (router)</a></td>\n</tr>\n<tr>\n<td>-</td>\n<td>-</td>\n<td>Optional, but parts of the guide assume a router with <a href=\"https://en.wikipedia.org/wiki/Border_Gateway_Protocol\">BGP</a> support</td>\n</tr>\n<tr>\n<td>$89</td>\n<td>1x</td>\n<td>(optional) <a href=\"https://store.ubnt.com/collections/wireless/products/unifi-ac-lite\">Unifi Wireless AC Lite</a></td>\n</tr>\n<tr>\n<td>$21</td>\n<td>1x</td>\n<td>(optional) <a href=\"https://www.amazon.com/gp/product/B00A121WN6/\">Any 8 port switch</a></td>\n</tr>\n<tr>\n<td>---</td>\n<td>---</td>\n<td>---</td>\n</tr>\n<tr>\n<td>$685</td>\n<td>-</td>\n<td>total</td>\n</tr>\n</tbody>\n</table>\n<p>Yikes, that is a large price tag.\nHere's a few ways to cut down the cost:</p>\n<ul>\n<li>\n<p>Buy fewer Raspberry Pis</p>\n<ul>\n<li>You'll want at least 3 to avoid the cluster running out of memory</li>\n</ul>\n</li>\n<li>\n<p>Use ethernet cables, switches, etc you already have laying around</p>\n<ul>\n<li>Most of the hardware above was picked because it's the same physical size as the Raspberry Pi, but this is only an aesthetic choice</li>\n</ul>\n</li>\n<li>\n<p>Skip the case</p>\n<ul>\n<li>just tape those Pis to a cardboard box, I won't judge</li>\n</ul>\n</li>\n<li>\n<p>Use your current router rather than the Unifi networking equipment and 8-port switch</p>\n<ul>\n<li>Your existing router will work fine for this setup with a couple small limitations</li>\n<li>I'll note later in the guide when the Unifi Router is required</li>\n</ul>\n</li>\n</ul>\n<p>Even with these cost cutting steps, I realize the price will be a non-starter for many people.\nI'd still recommend skimming the guide, hopefully still some interesting learnings even if you don't deploy it yourself.</p>\n<h2 id=\"networking\"><a href=\"#networking\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Networking</h2>\n<h3 id=\"draw-it-out\"><a href=\"#draw-it-out\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Draw it out</h3>\n<p>Here's a networking diagram of what we'll be building:</p>\n<img src=\"https://storage.googleapis.com/ansible-assets/k8s-rpi-networking.png\" alt=\"Network Diagram\" title=\"Network Diagram\" width=\"100%\">\n<p>We have split the machines into two subnetworks, LAN1 and LAN2.\nThe k8s cluster will live in LAN1 and all other machines (desktop, laptop, phone, etc) will live on LAN2.\nThe Router acts a bridge, allowing machines in LAN1 to talk to machines in LAN2 and vice versa.</p>\n<p>Let's zoom into LAN1 to examine networking within the k8s cluster.\nDon't worry if you don't recognize all the terms immediately, we'll introduce them in the upcoming sections.</p>\n<img src=\"https://storage.googleapis.com/ansible-assets/k8s-rpi-forwarding.png\" alt=\"Network Forwarding Diagram\" title=\"Network Forwarding Diagram\" width=\"100%\">\n<blockquote>\n<p>Note: Switch present but not pictured, only 3 workers pictured</p>\n</blockquote>\n<h3 id=\"example-deploy-a-vpn-service\"><a href=\"#example-deploy-a-vpn-service\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example: deploy a VPN Service</h3>\n<p>For this example, let's say we wanted to deploy a VPN server into the k8s cluster.\nWe'll cover the specific installation steps down in the Initial Setup section, this section will introduce the concepts.\nWhen you deploy an app to k8s, it runs as one or more <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod/\">pods</a> within the cluster.\nA pod is a group of one or more containers that should be grouped together on the same worker node.\nEach pod has a replica count which tells k8s how many copies to create of each pod.\nIf we specify three replicas in this example, k8s would create a VPN pod on each of the three workers.</p>\n<p>After the deploy completes, we have three VPN server processes running but we need some way to route traffic to them.\nFirst we'll create a k8s <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">service</a> to define how we will access the pods.\nThere are several types of services support by k8s, we'll cover a few in this guide:</p>\n<ul>\n<li>\n<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address\">ClusterIP</a></p>\n<ul>\n<li>(default) Service is given a cluster-internal IP address and is accessible only to other apps within the cluster</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#nodeport\">NodePort</a></p>\n<ul>\n<li>Service listens on a static port on the host machine, usually a high port like 30000</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer\">LoadBalancer</a></p>\n<ul>\n<li>Service is exposed via some infrastructure load balancer service. Often this is a cloud-provider specific service like AWS ELBs</li>\n</ul>\n</li>\n<li>\n<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\">External IPs</a></p>\n<ul>\n<li>Used in conjunction with any of the previous service types</li>\n<li>Specifying an <code>externalIP</code> for a service will cause all worker nodes to start listening on that service's <code>port</code>.\nIf a worker receives traffic on that port and the destination IP of that packet matches the <code>externalIP</code> of a service,\nthe worker will route that packet to the service's pod via the kube-proxy process.</li>\n<li>Allows you to bind on low ports like 80 and 443 but requires a k8s user with elevated privileges</li>\n</ul>\n</li>\n</ul>\n<p>General rule of thumb when choosing Service type:</p>\n<ul>\n<li>Choose ClusterIP if the service will only be accessed from within the cluster</li>\n<li>Choose NodePort if the service should be externally accessible but you don't have Load Balancer infrastructure in place and you don't need a privileged port like 80 or 443</li>\n<li>Choose ClusterIP with ExternalIP if the service should be externally accessible but you don't have Load Balancer infrastructure in place and want a privileged port </li>\n<li>Choose LoadBalancer if you want traffic balanced across all worker nodes containing pods for that service</li>\n</ul>\n<h3 id=\"load-balancing-with-metallb\"><a href=\"#load-balancing-with-metallb\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Load Balancing with MetalLB</h3>\n<p>On a minimal k8s install we'd have to use either NodePort or ClusterIP+ExternalIP to expose a service externally.\nThis has the drawback that a single worker node would receive all traffic for a given service.\nIf that worker goes down your service is inaccessible until that worker comes back up.\nTo overcome this limitation we'll deploy <a href=\"https://metallb.universe.tf/\">MetalLB</a> to handle creation of LoadBalancer services.\nMetalLB is a k8s load balancer implementation for bare metal setups like our Raspberry Pis.\nMetalLB has two operating modes: <a href=\"https://metallb.universe.tf/concepts/layer2/\">Layer 2 mode</a> and <a href=\"https://metallb.universe.tf/concepts/bgp/\">BGP mode</a></p>\n<h4 id=\"bgp-mode-recommended\"><a href=\"#bgp-mode-recommended\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BGP Mode (recommended)</h4>\n<blockquote>\n<p>Note: this section requires a Unifi Router or other router which supports BGP routing</p>\n</blockquote>\n<p>BGP stands for <a href=\"https://en.wikipedia.org/wiki/Border_Gateway_Protocol\">Border Gateway Protocol</a>.\nBGP allows two machines to exchange routing information, something like \"IP 1.2.3.4 is one hop away from IP 5.6.7.8\".\nThis protocol is used on a global scale by Internet Service Providers (ISPs) to figure out how to route traffic between ISPs.\nBut we're going to use the same protocol on a tiny scale to load balance traffic between our router and k8s worker nodes.</p>\n<p>We'll start by deploying MetalLB into our k8s cluster, configuring it with BGP options so that it can report routes to our Unifi router.\nMetalLB will watch for new services of type <code>LoadBalancer</code>, assign an IP to that service, and start publishing routes for that IP.\nFor example, let's say we had three workers (IPs 192.168.1.101, 192.168.1.102, and 192.168.1.103) and one VPN pod on the first worker.\nWe then create a Load Balancer service, causing MetalLB to assign that service an IP address of 192.168.1.200.\nMetalLB will then tell the router that shortest route to 192.168.1.200 is via 192.168.1.101 (the first worker's IP).\nIf the router receives a request for 192.168.1.200, it will route that traffic to the first worker node and that worker will\nsend the traffic to its VPN pod.\nIf we then scale up to three VPN pods (one on each worker), MetalLB will tell the router that\nthat shortest route to 192.168.1.200 is via either 192.168.1.101, 192.168.1.102, or 192.168.1.103.\nSince all routes have the same cost, the router will load balance requests across all three workers.\nWe have load balancing in our home network!</p>\n<p>However, there in one gotcha to this setup:\nmachines within the same subnet will not be able to resolve the Load Balancer IP addresses.\nThis is because machines within the same subnet can route packets directly to each other without going through the router,\nthe router is only needed to route packets between different subnets.\nSince the BGP route table is only present on the router, if the k8s cluster in on LAN1 and your laptop in on LAN1 you won't\nbe able to route traffic to that Load Balancer IP.\nTo overcome this, we divided our network into two subnets: LAN1 and LAN2.\nThe k8s cluster lives in LAN1 and everything else lives in LAN2.\nThis ensures that any requests from machines in LAN2 must go through the router which ensures the BGP routes are used.\nThe k8s machines can also route traffic to the Load Balancer IPs as each node has a MetalLB process on it which\nadds <code>iptables</code> rules for each service to the host machine.\nYou can also side-step this limitation by always sending requests to your modem's public IP address and adding a port\nforwarding rule to your router. We'll cover this setup shortly.</p>\n<h4 id=\"layer-2-mode-use-if-your-router-doesnt-have-bgp-support\"><a href=\"#layer-2-mode-use-if-your-router-doesnt-have-bgp-support\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Layer 2 Mode (use if your router doesn't have BGP support)</h4>\n<p>Layer 2 refers to the <a href=\"https://en.wikipedia.org/wiki/Data_link_layer\">Data Link layer</a> of the\n<a href=\"https://en.wikipedia.org/wiki/OSI_model\">Open Systems Interconnection (OSI) model</a> of describing\ncomputer networks.\nThis layer is concerned with how machines within a single subnet communicate with each other.\nThe advantage of deploying MetalLB in Layer 2 mode rather than BGP mode is that Layer 2 mode doesn't require\nany special networking hardware.</p>\n<p>Let's replay our previous example with three worker nodes and one VPN pod on the first worker.\nMetalLB again assigns the service the IP 192.168.1.200.\nIn Layer 2 mode, MetalLB will use the <a href=\"https://en.wikipedia.org/wiki/Address_Resolution_Protocol\">Address Resolution Protocol (ARP)</a>\nto advertise to other machines in that subnet that the first worker node also has the IP address 192.168.1.200 in addition to\nits original IP of 192.168.1.101.\nAny machine within that subnet can then route traffic to 192.168.1.200.\nIf we scale up to three VPN pods (one on each worker), MetalLB still only advertises the first worker's IP.\nBut when that worker receives traffic with a destination IP of 192.168.1.200,\nit will load balance traffic across all pods in the cluster (even those on other workers) via the kube-proxy process.\nSo it isn't true load balancing as a single worker node has to initially receive all the traffic for\na given service, but traffic is balanced across all pods from there.\nMetalLB will also automatically failover if that worker node goes down, giving a different worker that\nservice's IP.\nUnlike BGP mode, this setup requires all machines live on the same subnet.</p>\n<blockquote>\n<p>Note: the ansible steps listed below currently only support BGP mode for MetalLB,\nbut you can change that role to match the layer 2 configuration shown\n<a href=\"https://metallb.universe.tf/configuration/#layer-2-configuration\">here</a>.\nSend me a PR to make it configurable if you do!</p>\n</blockquote>\n<h3 id=\"accessing-services-from-outside-the-network\"><a href=\"#accessing-services-from-outside-the-network\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Accessing services from outside the network</h3>\n<p>After deploying MetalLB, we now have an IP address for our service which load balances traffic.\nHowever this address is only resolvable from within our home network.\nFor some services we'd like to access them from the office or when traveling.\nFor example, pointing my VPN client to <code>vpn.cats-are-cool.com:1194</code> should route the traffic\nto my home modem's public IP address, which passes the traffic to my router, which has a port\nforwarding rule to direct traffic on port <code>1194</code> to the MetalLB address <code>192.168.1.200</code>,\nwhich finally routes traffic into one of the VPN pods.\nLet's dive into the steps to make this happen automatically.</p>\n<h4 id=\"managing-dns-records\"><a href=\"#managing-dns-records\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Managing DNS records</h4>\n<p>For this setup, we'd like a single <a href=\"https://en.wikipedia.org/wiki/Wildcard_DNS_record\">wildcard DNS record</a> which resolves to our modem's public IP.\nThis guide uses <a href=\"https://www.cloudflare.com/\">CloudFlare</a> as the DNS provider, but other\nproviders should follow similar steps.\nWe'd start by looking up our modem's public IP (hint: google \"what's my ip\").\nThen we'll go our DNS provider and create an <a href=\"https://support.dnsimple.com/articles/a-record/\">A record</a>\nfor <code>*.cats-are-cool.com</code> pointing to our public IP and a Time-to-Live (TTL) of 2 minutes (shortest value you can).\nThis means that requests for any subdomain of <code>cats-are-cool.com</code> (like <code>vpn.cats-are-cool.com</code>) will be routed\nto your modem/router.\nHowever, normally residential networks have ephemeral public IP addresses, meaning it can change\nat any time.\nAnd we'd like to avoid manual creation of DNS records anyway.</p>\n<p>To handle the creating and updating of this wildcard record, the setup steps below include a\n<code>dns-updater</code> <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/\">CronJob</a>\nwhich creates the wildcard record if it doesn't exist and updates the record's target IP\naddress if your public IP has changed.\nThis job runs every 5 minutes as a pod within the k8s cluster.</p>\n<blockquote>\n<p>Note: the ansible steps listed below currently only support Cloudflare as a DNS provider, but can be tweaked to support\nother providers</p>\n</blockquote>\n<h4 id=\"managing-port-forwarding-rules\"><a href=\"#managing-port-forwarding-rules\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Managing port forwarding rules</h4>\n<p>Next we'll need to add port forwarding rules to our router.\nForwarding rules are a mapping of port to IP address.\nWhen the router receives a request on a given port, it will check its list of port forwarding rules\nto see if it has a rule for that port.\nIf it does it will forward the traffic to the IP address listed in that rule.\nFor the VPN example, we need to add a port forwarding for port <code>1194</code> and IP <code>192.168.1.200</code> (the MetalLB service IP).\nNormally you do this via your router's UI page, accessible at <code>http://192.168.1.1</code> usually.</p>\n<p>However, if you've got a Unifi router we can manage these port forwarding rules automatically.\nI created a custom k8s controller called the <a href=\"https://github.com/ljfranklin/port-forwarding-controller\">port-forwarding-controller</a>.\nYou deploy this controller into your cluster with credentials to talk to your Unifi router and\nit watches for new services similar to MetalLB.\nWhen the controller sees a new or updated service, it checks whether the Unifi controller has a forwarding rule\nmatching that service's IP and port.\nIf no rule exists, the controller will create it automatically.\nWith this last bit in place, requests to <code>vpn.cats-are-cool.com</code> should be forwarded successfully into the VPN pods.</p>\n<blockquote>\n<p>Again, the port-forwarding-controller currently only supports the Unifi API. PRs accepted!</p>\n</blockquote>\n<h4 id=\"handling-https-traffic\"><a href=\"#handling-https-traffic\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Handling HTTPS traffic</h4>\n<p>In our VPN example, our service used the port <code>1194</code> and any traffic\nto that port went to the VPN pods.\nThis worked well, but what if you wanted several different services to receive HTTPS traffic?\nYou could have each one listen on a different port, but it'd be nice if they could all use the standard 443 port.\nTo give a specific example, we'd like requests to <code>https://homepage.cats-are-cool.com</code>\nto go to a homepage app while requests to <code>https://passwords.cats-are-cool.com</code> to go to a password manager.</p>\n<p>To accomplish this, k8s provides an <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> resource\nto create a mapping between requests and services.\nFor example, you can create an Ingress resource that says any request containing the hostname <code>homepage.cats-are-cool.com</code>\nshould be routed to the <code>homepage</code> Service.\nHowever, creating an Ingress resource doesn't do anything on its own, you need an ingress controller.\nWe'll deploy the <a href=\"https://kubernetes.github.io/ingress-nginx/\">ingress-nginx-controller</a> to manage these objects for us.\nThis controller watches for new and updated Ingress resources to build up a mapping of request options to target services.\nWhen the controller itself receives a request, it checks this mapping to see where to forward the traffic.\nThis means the ingress controller is deployed between MetalLB and a target service like the homepage app.\nWe'll need to create a Load Balancer service in front of the ingress controller so that a Load Balancer IP and port forwarding\nrules are created.</p>\n<p>Let's walk through an example HTTPS request end-to-end by visiting <code>https://homepage.cats-are-cool.com</code> from outside the network.\nAs with the VPN example, this request reaches our modem's public IP which passes the traffic to the router.\nThe router has a port forwarding rule to forward traffic on port <code>443</code> to the MetalLB address <code>192.168.1.201</code>.\nThat address was assigned to the ingress-controller so MetalLB forwards the request to the ingress-controller pod.\nThe ingress-controller sees that the target hostname of the request matches an Ingress resource\nwhose target service is the <code>homepage</code> service.\nThis match causes the controller to forward the traffic to one of the <code>homepage</code> pods.</p>\n<h4 id=\"generating-tls-certificates\"><a href=\"#generating-tls-certificates\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Generating TLS certificates</h4>\n<p>Handling HTTPS traffic comes with another complication: generating TLS certificates.\nCertificates are used by HTTPS clients like web browsers to verify the identity of the requested website.\nCertificates are issued by Certificate Authorities (CAs) which are trusted by your browser.\nBy default, most k8s HTTPS services will deploy with self-signed TLS certificates which are not trusted.\nVisiting a site with a self-signed certificate in your browser will show a scary \"Connection is not secure\" warning\nas your browser can't verify that site's identity.</p>\n<p>To automatically generate trusted certificates, we'll use a component called <a href=\"https://github.com/jetstack/cert-manager\">cert-manager</a>\nto get certificates from the free <a href=\"https://letsencrypt.org/\">Let's Encrypt CA</a>.\nYou start by deploying cert-manager with credentials for your DNS provider (we'll use CloudFlare again) as well as your Let's Encrypt email.\nOnce deployed, cert-manager looks for Ingress resources with a <code>tls</code> key as shown <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/#tls\">here</a>.\nFor each <code>tls</code> key, cert-manager will make a certificate request to a Let's Encrypt server to generate a\ncertificate for each DNS record listed in <code>tls.hosts</code>.\nThe Let's Encrypt server will then ask cert-manager to prove it owns the requested DNS records by\npushing a bit of DNS metadata to the DNS provider.\nOnce Let's Encrypt verifies that the requested DNS metadata was added, it will return the requested certificates.\ncert-manager will then take that certificate and store it in a k8s <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\">Secret</a>.\nThis secret can later be mounted into a web server's container as a file.\nLet's Encrypt certificates are only valid for 90 days, but cert-manager will automatically renew any certificates that are\nnearing their expiration.</p>\n<blockquote>\n<p>Note: Let's Encrypt has strict <a href=\"https://letsencrypt.org/docs/rate-limits/\">rate limits</a> for how many certificates it will generate.\nDuring your initial testing you can use the staging Let's Encrypt server (<a href=\"https://acme-staging-v02.api.letsencrypt.org/directory\">https://acme-staging-v02.api.letsencrypt.org/directory</a>) rather\nthan the production one to test your setup with fake certs until you get it working.</p>\n</blockquote>\n<h2 id=\"storage\"><a href=\"#storage\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Storage</h2>\n<p>With the previous steps in place, we're able to deploy stateless apps to our k8s cluster.\nBut some apps need to store persistent data that is still present after a reboot.\nk8s defines a <a href=\"https://kubernetes.io/docs/concepts/storage/volumes\">Volume</a> resource to allow persistent data to be\nmounted into the container.</p>\n<p>On a single node bare metal k8s cluster, you could use a <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\">hostPath</a>\nvolume which mounts a directory from the underlying worker machine into the container.\nWe have a multi-node cluster so this won't work.</p>\n<p>In a multi-node cluster we could instead use a <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#local\">Local Volume</a>.\nThis is similar to a <code>hostPath</code> volume in that it mounts a disk or directory from the underlying host machine into the\ncontainer, but this is supported in a multi-node cluster.\nThis works, but still has a couple limitations:</p>\n<ul>\n<li>Volumes must be manually created prior to deploying the app</li>\n<li>All pods that use the volume are locked to a single worker node</li>\n<li>If the worker goes down the app will be inaccessible until the worker comes back</li>\n</ul>\n<p>To overcome this limitation, we're going to use <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#dynamic\">Dynamic Volumes</a>.\nWith Dynamic Volumes, a deployment can create a <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes\">Persistent Volume Claim (PVC)</a>\nto request a volume of a given size.\nYour cluster's volume provider watches for these claims and creates volumes on demand.\nOnce the volume is ready, the pod that requested it will be created and the volume will be mounted into the container.</p>\n<p>For this bare metal setup, we'll use <a href=\"https://docs.gluster.org/en/latest/\">GlusterFS</a> and <a href=\"https://github.com/heketi/heketi/wiki\">Heketi</a>\nas our volume provider.\nGlusterFS is a multi-node network filesystem and Heketi is an API layer to manage GlusterFS volumes.\nLater on we'll use the <a href=\"https://github.com/gluster/gluster-kubernetes\">gluster-kubernetes</a> project to configure\nthese components to handle provisioning of Volumes in our cluster.\nIn the steps shown below we'll plug a USB drive into one of our worker nodes.\nThis worker will run the GlusterFS server and all persistent data will be stored on the USB drive.\nHowever, these volumes can be mounted over the network by containers running on other workers.\nIf you want, you can buy a couple more USB drives and deploy GlusterFS onto multiple nodes\nto add replicate data and increase availability.</p>\n<p>Once we have GlusterFS+Heketi deployed, we can start using dynamic persistent volumes.\nOur VPN deployment might create a Persistent Volume Claim of size 5GB.\nOur volume provider will notice this new claim and create a 5GB volume in Gluster filesystem.\nOnce the Volume is ready, k8s will continue creating the VPN container and mount the\nVolume into the container.\nIf the VPN container is deleted, the volume will be re-attached to the new container and\nthe existing data will still be present.</p>\n<h2 id=\"installing-software-with-helm\"><a href=\"#installing-software-with-helm\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Installing software with Helm</h2>\n<p>While k8s is an incredibly flexible tool, it requires a lot of boilerplate configuration to get apps running.\nAs an extreme example, the <a href=\"https://github.com/knative/serving/releases/download/v0.2.2/release.yaml\">deployment manifest</a>\nfor the tool Knative is 17 thousand lines long.\nTo avoid most of these boilerplate YAML files, the setup below includes a tool called <a href=\"https://helm.sh/\">Helm</a> to make\ninstalling software into k8s a bit easier.</p>\n<p>Helm lets you install software from a set of <a href=\"https://github.com/helm/charts\">charts</a>.\nFor example, to install a VPN server with helm you run:</p>\n<pre><code>helm install stable/openvpn\n</code></pre>\n<p>In a few seconds you have a running VPN server running in your cluster.\nTo tweak some configuration options, you can create a YAML file containing the options defined in that\n<a href=\"https://github.com/helm/charts/tree/master/stable/openvpn#configuration\">chart's README</a>:</p>\n<pre><code>helm install --values openvpn-options.yml stable/openvpn\n</code></pre>\n<h2 id=\"initial-setup\"><a href=\"#initial-setup\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Initial Setup</h2>\n<p>Now that we understand the concepts, let's start deploying it.</p>\n<h3 id=\"hardware-setup\"><a href=\"#hardware-setup\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hardware Setup</h3>\n<img src=\"https://storage.googleapis.com/ansible-assets/k8s-rpi-cover.png\" alt=\"Hardware\" title=\"Hardware\" width=\"100%\">\n<p>Steps to setup hardware:</p>\n<ul>\n<li>Plug Unifi Gateway's WAN1 port into your modem</li>\n<li>\n<p>Plug Unifi Gateway's LAN1 port into the larger switch</p>\n<ul>\n<li>Note: the picture shows some components plugged into LAN2, we'll cover this in a later section</li>\n</ul>\n</li>\n<li>\n<p>The Unifi AP should come with a Power-over-Ethernet (PoE) adapter</p>\n<ul>\n<li>Plug the LAN port of the adapter into the larger switch</li>\n<li>Plug the PoE port of the adapter into the Unifi AP</li>\n</ul>\n</li>\n<li>\n<p>Assemble the Raspberry Pi's into the stacking case</p>\n<ul>\n<li>The case linked above includes a 7th level that we won't use</li>\n</ul>\n</li>\n<li>\n<p>Plug USB cables into the charging dock but don't plug then into the Raspberry Pi's yet</p>\n<ul>\n<li>We'll power up the Pi's after we flash the SD cards</li>\n</ul>\n</li>\n<li>Plug the USB power cable of the mini switch into a USB port on one of the Pi's</li>\n<li>Plug each Pi into the mini switch using the mini ethernet cables</li>\n<li>Plug the mini switch into the larger switch using another mini ethernet cable</li>\n<li>Plug everything into AC power</li>\n</ul>\n<blockquote>\n<p>Note: you'll need a wired ethernet connection on your workstation for now.\nLater steps will install the Unifi Controller into the k8s cluster which will allow you\nto setup the Wifi network.</p>\n</blockquote>\n<h3 id=\"ansible-introduction\"><a href=\"#ansible-introduction\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ansible Introduction</h3>\n<blockquote>\n<p>Note: The following steps assume you already have <code>git</code> and <code>python</code> installed.</p>\n</blockquote>\n<p>We'll use <a href=\"https://www.ansible.com/\">Ansible</a> to deploy the cluster.\nAnsible works by running commands over SSH from your workstation to each node\nin the Raspberry Pi cluster.\nAnsible has many features, but we'll introduce three basics here: inventory, roles, and playbooks.</p>\n<p>An inventory file lists the IP addresses of each machine you want to provision and\ngroups each machine by responsibility.\nThis file looks similar to the following:</p>\n<pre><code>[all]\nk8s-node1 ansible_host=192.168.1.100\nk8s-node2 ansible_host=192.168.1.101\nk8s-node3 ansible_host=192.168.1.102\n\n[kube-master]\nk8s-node1\n\n[kube-node]\nk8s-node2\nk8s-node3\n</code></pre>\n<p>This inventory file describes a three node k8s cluster.\nAll the machines are listed at the top under the <code>all</code>,\nthe first machine will serve as the master node,\nand the other two machines will be worker nodes.</p>\n<p>A role is a set of configuration options, files, and commands to run on a target machine.\nFor example, the <code>upgrade-master</code> role will update k8s to the specified version while\nthe <code>openvpn</code> role will deploy a VPN container into the cluster.\nA role will usually contain a <code>tasks/main.yml</code> file which lists which commands to run,\na <code>templates</code> directory containing files to transfer to the target machine,\nand a <code>defaults/main.yml</code> file which lists supported configuration options.</p>\n<p>Finally, playbooks will tie the inventory and roles together.\nHere's a playbook example:</p>\n<pre><code>- hosts: all\n  roles:\n    - glusterfs-client\n\n- hosts: gfs-cluster\n  roles:\n    - glusterfs-server\n</code></pre>\n<p>This playbook tells Ansible to run the <code>glusterfs-client</code> role on all machines,\nthen run the <code>glusterfs-server</code> role only on machines in the <code>gfs-cluster</code> inventory group.</p>\n<h3 id=\"create-project\"><a href=\"#create-project\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Create project</h3>\n<p>Create a new repository:</p>\n<pre><code>mkdir k8s-pi\ncd k8s-pi\ngit init\n</code></pre>\n<p>Create <code>.gitignore</code>:</p>\n<pre><code>cat &#x3C;&#x3C; EOF > .gitignore\n/secrets/\n/tmp/\n*.retry\nEOF\n</code></pre>\n<p>Clone <code>k8s-pi</code> repo as a submodule:</p>\n<pre><code>mkdir submodules\ngit submodule add https://github.com/ljfranklin/k8s-pi.git ./submodules/k8s-pi\n</code></pre>\n<h3 id=\"flash-hypriotos-onto-sd-cards\"><a href=\"#flash-hypriotos-onto-sd-cards\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Flash HypriotOS onto SD cards</h3>\n<blockquote>\n<p>Why HypriotOS?\nThis Debian-based distribution is optimized for running Docker workloads such as a k8s cluster and\ncomes with many container utilities pre-installed.\nIt also includes a tool called <code>cloud-init</code> which allows us to specify options like SSH keys and\nstatic IPs in a config file on the SD card rather than running setup commands over SSH after boot.</p>\n</blockquote>\n<p>Create an SSH key to access the Raspberry Pi's:</p>\n<pre><code>ssh-keygen -t rsa -b 4096 -C k8s -N '' -f ~/.ssh/id_rsa_k8s\nssh-add ~/.ssh/id_rsa_k8s\n</code></pre>\n<p>Plug a microSD card into your workstation (this example assumes the card has the device ID <code>/dev/sda</code>), then run the following command:</p>\n<pre><code>./submodules/k8s-pi/pi/provision.sh -d /dev/sda -n k8s-node1 -p \"$(cat ~/.ssh/id_rsa_k8s.pub)\" -i 192.168.1.100\n</code></pre>\n<blockquote>\n<p>Note: the SD card must be unmounted prior to running the script</p>\n</blockquote>\n<p>Unplug the microSD card and plug in the next one. Run the script again but increment the node number and IP:</p>\n<pre><code>./submodules/k8s-pi/pi/provision.sh -d /dev/sda -n k8s-node2 -p \"$(cat ~/.ssh/id_rsa_k8s.pub)\" -i 192.168.1.101\n</code></pre>\n<p>Repeat the process until all cards have been flashed.</p>\n<h3 id=\"boot-raspberry-pis\"><a href=\"#boot-raspberry-pis\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boot Raspberry Pi's</h3>\n<p>Plug all the cards into the Raspberry Pi's and attach the USB power cable to start them up.\nThe Raspberry Pi's will install necessary packages on boot and will automatically reboot once to finish a kernel update.</p>\n<p>Shortly after this reboot you should be able to SSH onto each node:</p>\n<pre><code>ssh k8s@192.168.1.100\n</code></pre>\n<p>We'll also plug in a USB drive on the last node to store our Persistent Volumes.\nEnsure the drive is plugged in but unmounted and run the following command to remove any existing filesystems:</p>\n<pre><code>ssh k8s@192.168.1.105\nsudo wipefs -a /dev/sda # assumes USB drive has device ID /dev/sda\n</code></pre>\n<blockquote>\n<p>Note: you can add USB drives to multiple nodes for extra redundancy if you want.\nRemember to add these hosts under the <code>[gfs-cluster]</code> section in <code>hosts.ini</code> as shown below.</p>\n</blockquote>\n<h3 id=\"installing-k8s\"><a href=\"#installing-k8s\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Installing k8s</h3>\n<p>Create an <code>ansible.cfg</code> file in project root:</p>\n<pre><code>cat &#x3C;&#x3C; EOF > ansible.cfg\n[defaults]\nhost_key_checking     = False\nremote_user           = k8s\nroles_path            = submodules/k8s-pi/roles/\n\n[ssh_connection]\npipelining        = True\nssh_args          = -o ControlMaster=auto -o ControlPersist=30m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\nEOF\n</code></pre>\n<p>Install Ansible + deps:</p>\n<pre><code>sudo pip install -r submodules/k8s-pi/requirements.txt\n</code></pre>\n<p>Create an inventory file containing host information:</p>\n<pre><code>mkdir inventory\n\ncat &#x3C;&#x3C; EOF > inventory/hosts.ini\n[all]\nk8s-node1 ansible_host=192.168.1.100\nk8s-node2 ansible_host=192.168.1.101\nk8s-node3 ansible_host=192.168.1.102\nk8s-node4 ansible_host=192.168.1.103\nk8s-node5 ansible_host=192.168.1.104\nk8s-node6 ansible_host=192.168.1.105\n\n[kube-master]\nk8s-node1\n\n[kube-node]\nk8s-node2\nk8s-node3\nk8s-node4\nk8s-node5\nk8s-node6\n\n[gfs-cluster]\nk8s-node6 volume_device=/dev/sda\nEOF\n</code></pre>\n<blockquote>\n<p>Note: this config file assumes 6 Raspberry Pis with sequential static IP addresses starting at 192.168.1.100.\nIt also assumes that the last node has a USB drive which we'll use for persistent storage of volumes.\nAdjust the number of nodes and IP addresses to suit your setup.</p>\n</blockquote>\n<p>Create three playbook files: <code>bootstrap.yml</code> (setup k8s from scratch), <code>upgrade.yml</code> (upgrade k8s), <code>deploy.yml</code> (deploy k8s services only):</p>\n<pre><code>cat &#x3C;&#x3C; EOF > bootstrap.yml\n- name: Include k8s-pi bootstrap tasks\n  import_playbook: submodules/k8s-pi/bootstrap.yml\nEOF\n\ncat &#x3C;&#x3C; EOF > upgrade.yml\n- name: Include k8s-pi upgrade tasks\n  import_playbook: submodules/k8s-pi/upgrade.yml\nEOF\n\ncat &#x3C;&#x3C; EOF > deploy.yml\n- name: Include k8s-pi deploy tasks\n  import_playbook: submodules/k8s-pi/deploy.yml\nEOF\n</code></pre>\n<p>Create a <code>secrets.yml</code> file and fill in your credentials for the deployed services:</p>\n<pre><code>mkdir -p secrets\ncp submodules/k8s-pi/secrets/secrets.sample secrets/secrets.yml\n</code></pre>\n<p>Run the <code>bootstrap.yml</code> playbook to start the installation:</p>\n<pre><code>ansible-playbook -i inventory/hosts.ini --extra-vars @secrets/secrets.yml bootstrap.yml\n</code></pre>\n<blockquote>\n<p>Important! Running <code>bootstrap.yml</code> playbook a second time will wipe all data from the cluster.</p>\n</blockquote>\n<p>You should now have a running k8s cluster!\nTo interact with the cluster, install the <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\">kubectl CLI</a> and\ncopy the kubectl config file into your home directory:</p>\n<pre><code>cp ./secrets/admin.conf ~/.kube/config\n</code></pre>\n<p>To check that it's working run <code>kubectl -n kube-system get pods</code>.</p>\n<p>This repo also includes two other playbooks: <code>upgrade.yml</code> and <code>deploy.yml</code>.\nTo upgrade k8s without wiping data, run the <code>upgrade.yml</code> playbook.\nTo upgrade just the apps running on k8s, run the <code>deploy.yml</code> playbook.</p>\n<h3 id=\"optional-setup-unifi-controller\"><a href=\"#optional-setup-unifi-controller\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Setup Unifi Controller</h3>\n<blockquote>\n<p>Skip if you don't have a Unifi Router</p>\n</blockquote>\n<p>After following the above steps, a Unifi Controller should be running as a pod in your k8s cluster.\nThe following steps are required to ensure the Router and AP are adopted by the Controller.\nAfter adopting both devices you will be able to configure settings like the Wifi network by\nvisiting <code>https://unifi.$INGRESS_DOMAIN</code>.</p>\n<ul>\n<li>\n<p>Ensure all devices are plugged into LAN1 port of Unifi Gateway</p>\n<ul>\n<li>We'll switch some devices over to LAN2 in a later step to enable BGP routing</li>\n</ul>\n</li>\n<li>\n<p>Temporarily modify <code>/etc/hosts</code> on your workstation to route Controller traffic directly to the first worker node:</p>\n<ul>\n<li><code>sudo vim /etc/hosts</code> to add <code>$FIRST_WORKER_NODE_IP unifi.$INGRESS_DOMAIN</code></li>\n<li>Note: We'll revert this change once the Unifi controller is setup with BGP routing</li>\n</ul>\n</li>\n<li>\n<p>Visit <code>https://unifi.$INGRESS_DOMAIN</code></p>\n<ul>\n<li>Configure Wifi network name/password and controller/device username/passwords</li>\n</ul>\n</li>\n<li>\n<p>Adopt the Unifi Gateway (detailed steps <a href=\"https://help.ubnt.com/hc/en-us/articles/204909754-UniFi-Device-Adoption-Methods-for-Remote-UniFi-Controllers#8\">here</a>):</p>\n<ul>\n<li><code>ssh ubnt@192.168.1.1</code> (password <code>ubnt</code>)</li>\n<li>If gateway was previously paired:</li>\n<li><code>sudo syswrapper.sh restore-default</code></li>\n<li>SSH session may get stuck, may need to kill it and re-SSH after reboot</li>\n<li><code>set-inform http://$FIRST_WORKER_IP:8080/inform</code></li>\n<li>Go to Controller UI and click Adopt on Devices tab</li>\n<li>Wait for device to go from <code>Adopting</code> to <code>Provisioning</code> to <code>Connected</code> on Controller UI</li>\n</ul>\n</li>\n<li>\n<p>Adopt the Unifi AP</p>\n<ul>\n<li>If the AP was not previously paired with another controller:</li>\n<li>The AP should appear in the Devices tab, click Adopt next to it in the UI</li>\n<li>If the AP was previously paired:</li>\n<li>Hold down the small Reset button on the AP with a paper clip to reset to factory default settings</li>\n<li>After reseting the AP should appear in the Devices tab, click Adopt</li>\n</ul>\n</li>\n<li>\n<p>Add temporary port forwarding rule to bootstrap <code>port-forwarding-controller</code>:</p>\n<ul>\n<li>Settings > Routing &#x26; Firewall > Port Forwarding > Create New Port Forwarding Rule</li>\n<li>Name: tmp-k8s-ingress</li>\n<li>Port: 443</li>\n<li>Forward IP: $ingress<em>nginx</em>static_ip (from secrets.yml)</li>\n<li>Forward Port: 443</li>\n<li>Save</li>\n</ul>\n</li>\n<li>\n<p>Restart the <code>port-forwarding-controller</code> to ensure it adds port forwarding rules to controller</p>\n<ul>\n<li><code>kubectl delete pod port-forwarding-0</code></li>\n<li>Verify rules were added under Settings > Routing &#x26; Firewall > Port Forwarding</li>\n<li>Delete <code>tmp-k8s-ingress</code> rule</li>\n</ul>\n</li>\n<li>Remove <code>$WORKER_IP</code> line from <code>/etc/hosts</code></li>\n<li>Done!</li>\n</ul>\n<h3 id=\"optional-move-non-k8s-machines-over-to-lan2-to-enable-bgp-routing\"><a href=\"#optional-move-non-k8s-machines-over-to-lan2-to-enable-bgp-routing\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Move non-k8s machines over to LAN2 to enable BGP routing</h3>\n<p>A limitation of this BGP routing setup is that machines on the 192.168.1.0/24 subnet cannot route to the BGP Load Balancer IP addresses.\nThis is due to machines on the same subnet wanting to route traffic directly to the target machine rather than sending traffic through the router first.\nTo force traffic to go through the router we can move all non-k8s machines over to a new LAN2 network.</p>\n<ul>\n<li>Start with all machines plugged into LAN1 port on Unifi Gateway</li>\n<li>\n<p>Visit <code>https://unifi.$INGRESS_DOMAIN</code></p>\n<ul>\n<li>Enable LAN2 network:</li>\n<li>Settings > Networks > Create New Network</li>\n<li>Name: LAN2</li>\n<li>Interface: LAN2</li>\n<li>Gateway/Subnet: 192.168.2.1/24</li>\n<li>DHCP Range: 192.168.2.6 - 192.168.2.254</li>\n<li>All other values default</li>\n<li>Save</li>\n</ul>\n</li>\n<li>Unplug the mini switch from the larger switch</li>\n<li>Unplug the larger switch from the router's LAN1 port and plug in into LAN2</li>\n<li>Plug the mini switch into the LAN2 port</li>\n<li>Connect all other machines (desktop, Wifi AP, etc.) into LAN2 switch</li>\n<li>\n<p>Verify that you can now route directly to BGP LB addresses:</p>\n<ul>\n<li><code>nc -v -z $ingress_nginx_static_ip 443</code></li>\n<li>Should say <code>Connection to 192.168.1.51 443 port [tcp/https] succeeded!</code> or similar</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"optional-access-k8s-dashboard\"><a href=\"#optional-access-k8s-dashboard\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Access K8S Dashboard</h2>\n<p>A dashboard for your k8s cluster should now be available at <code>https://k8s.$INGRESS_DOMAIN</code>.\nThis dashboard displays metrics like CPU and memory usage, as well as listing all deployed k8s resources.\nTo get a login token, run <code>./submodules/k8s-pi/scripts/get-dashboard-token.sh</code>.\nVisit the dashboard URL, select Token on the login prompt, and paste in the token returned by the script.</p>\n<h2 id=\"optional-connect-to-cluster-with-vpn\"><a href=\"#optional-connect-to-cluster-with-vpn\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Connect to cluster with VPN</h2>\n<p>A VPN server is now available at <code>vpn.$INGRESS_DOMAIN</code>.\nTo access it, generate a secret VPN config file by running this script:</p>\n<pre><code>./submodules/k8s-pi/scripts/generate-vpn-cert.sh vpn.$INGRESS_DOMAIN\n</code></pre>\n<p>This will create a <code>secrets/k8s.ovpn</code> file containing the necessary connection information.\nPass this file to your VPN client to connect to k8s internal services and cluster IPs even when\noutside your home network.</p>\n<h2 id=\"optional-backuprestore\"><a href=\"#optional-backuprestore\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Backup/Restore</h2>\n<p>It wouldn't be a real production-ish cluster if we didn't have automated backups.\nThe Ansible playbooks deploy the <a href=\"https://github.com/heptio/ark\">Ark</a> k8s backup/restore utility.\nWith the default configuration, Ark will take a backup of all k8s resources and persistent volumes once a day.\nThis means you can delete your entire cluster, restore from a backup, and all pods, services, and persistent\ndata will be restored just as it was when the last backup was taken.\nThe Ark deployment includes a component called <a href=\"https://restic.net/\">Restic</a> to take backups of GlusterFS volumes.\nThese backups will be stored in a Google Cloud Storage (GCS) bucket although you can configure it with other\nbackup services like S3.\nArk will also automatically remove backups that are more than two weeks old.</p>\n<h3 id=\"take-backup-on-demand\"><a href=\"#take-backup-on-demand\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Take backup on-demand</h3>\n<p>Backups are taken automatically once a day.\nTo take one on-demand:</p>\n<pre><code>ark create backup backup-01032019 --ttl 360h0m0s\n</code></pre>\n<p>This command takes a backup of the entire cluster.\nArk will automatically delete this backup after ~2 weeks (360 hours).\nYou can check on the progress of the backup by running this command:</p>\n<pre><code>ark describe backup backup-01032019 --volume-details\n</code></pre>\n<p>List all backups stored on GCS:</p>\n<pre><code>ark get backups\n</code></pre>\n<h3 id=\"restoring-entire-cluster-from-backup\"><a href=\"#restoring-entire-cluster-from-backup\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Restoring entire cluster from backup</h3>\n<p>First, get the name of the latest backup:</p>\n<pre><code>$ ark get backups\nNAME                           STATUS      CREATED                         EXPIRES   SELECTOR\ndaily-backups-20190103070021   Completed   2019-01-02 23:00:21 -0800 PST   13d       &#x3C;none>\ndaily-backups-20190102070021   Completed   2019-01-01 23:00:21 -0800 PST   12d       &#x3C;none>\n...\n</code></pre>\n<p>To start from a completely clean state,\nrepeat the steps above to wide all the SD cards.\nAlso remember to run <code>sudo wipefs -a /dev/sda</code> to clear out the USB drive used by GlusterFS.</p>\n<p>Comment out the <code>- import_playbook: deploy.yml</code> line of the <code>upgrade.yml</code> playbook to skip re-creating any k8s resources.\nUncomment the line <code>backup_restore_only_mode: true</code> in <code>secrets/secrets.yml</code> so that Ark does add or delete any existing backups.</p>\n<p>Now run the <code>bootstrap.yml</code> playbook to recreate the cluster from scratch.\nFinally restore all k8s resources from backup:</p>\n<pre><code>ark create restore restore-01032019 --from-backup backup daily-backups-20190103070021\n</code></pre>\n<p>You can check on the restore progress with:</p>\n<pre><code>ark describe restore restore-01032019 --volume-details\n</code></pre>\n<p>Undo your changes to <code>upgrade.yml</code> and <code>secrets.yml</code> and re-run the <code>ark</code> role to allow it to resume taking daily backups.</p>\n<h3 id=\"restoring-select-deployments-from-backup\"><a href=\"#restoring-select-deployments-from-backup\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Restoring select deployments from backup</h3>\n<p>If you'd like to restore only a subset of resources, e.g. only the VPN resources, specify a label selector on the restore:</p>\n<pre><code>ark create restore restore-01032019 --from-backup backup daily-backups-20190103070021 --label app=openvpn\n</code></pre>\n<h3 id=\"possible-gotcha-restic-repo-shows-notready\"><a href=\"#possible-gotcha-restic-repo-shows-notready\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Possible gotcha: Restic Repo shows NotReady</h3>\n<p>At one point after shutting down and restoring, my Ark deployment was unable to take new backups.\nThis was due to <code>ark restic repo get</code> returning <code>NotReady</code>.\nTurns out the Restic repository was still marked as \"locked\", possibly\ndue to not shutting down the cluster gracefully.\nRunning the following commands unlocked the repo:</p>\n<pre><code>kubectl -n heptio-ark exec -it ark-restic-POD_ID /bin/sh\nrestic unlock -r gs:&#x3C;VOLUME_BACKUP_BUCKET>:default\n# enter 'static-passw0rd' as the repo password\n</code></pre>\n<p>At time of writing, volume backups are encrypted with the <a href=\"https://github.com/heptio/ark/blob/9f72cf9c614bb4dc02dfacae08c9dcd11fbb5eaa/pkg/restic/repository_keys.go#L33\">hardcoded</a>\nkey 'static-passw0rd'. This may change in future releases.</p>\n<h2 id=\"optional-adding-your-own-ansible-tasks\"><a href=\"#optional-adding-your-own-ansible-tasks\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Adding your own Ansible tasks</h2>\n<p>The components deployed so far are just a starting point.\nYou'll want to deploy additional software into the cluster to suit your needs.\nTo do this you can create new Ansible roles and include these roles in the <code>deploy.yml</code> playbook.\nYou can start by copying an existing role ( e.g. <code>./submodules/k8s-pi/roles/openvpn</code>) into your own\n<code>./roles</code> directory and change the files to install a different piece of software.</p>\n<h2 id=\"optional-building-arm-images\"><a href=\"#optional-building-arm-images\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Optional: Building ARM images</h2>\n<p>You've worked hard to get to this point, but I've got some bad news.\nMany Helm charts do not support the ARM CPU architecture out-of-the-box.\nThe vast majority of computers you interact with day-to-day use the <code>x86_64</code> architecture,\nalso called <code>amd64</code>, while the Raspberry Pi's use the <code>arm</code> architecture, also called <code>armv7l</code>.</p>\n<blockquote>\n<p>Side note, the Raspberry Pi Model 3 B+ also supports <code>arm64</code> (a.k.a. <code>aarch64</code>), the 64 bit version of <code>arm</code>.\nHowever, since this model has less than 4GB of RAM you don't get the main benefit of using 64-bit software.\nAs a result the Raspberry Pi focused OS distributions don't have a 64-bit version at this time.</p>\n</blockquote>\n<p>Docker images which are used to create k8s containers must be built to support each individual architecture.\nMany Helm charts include images which are built only for <code>x86_64</code> architecture.\nIf you try to run these images in your Raspberry Pi cluster, the containers will fail to start with <code>Exec error</code>.\nYou have a couple options to get an image built for <code>arm</code>:</p>\n<ul>\n<li>Check whether the image maintainers created the <code>arm</code> image under a different name or tag, e.g. <code>k8s.gcr.io/defaultbackend-arm:1.4</code></li>\n<li>\n<p>Google around to see whether someone else has uploaded an <code>arm</code> image to Dockerhub for that piece of software</p>\n<ul>\n<li>Disclaimer: use at your own risk when using software from someone other than the official project maintainers</li>\n</ul>\n</li>\n<li>\n<p>Build the image yourself on a Raspberry Pi, here are the usual steps:</p>\n<ul>\n<li>clone the target project from GitHub onto the Raspberry Pi</li>\n<li><code>cd</code> to the directory containing the Dockerfile</li>\n<li>Run <code>docker build -t YOUR_USERNAME/IMAGE-arm:latest .</code></li>\n<li>Run <code>docker login</code> and paste in your Dockerhub credentials</li>\n<li>Run <code>docker push YOUR_USERNAME/IMAGE-arm:latest</code></li>\n<li>Specify the new image name and tag in the helm values file, the name of the key will vary by chart</li>\n</ul>\n</li>\n<li>Open an issue asking the project maintainers to start publishing <code>arm</code> images</li>\n</ul>\n<p>If you want to publish a single Docker image which runs on multiple architectures,\nyou can push an <a href=\"https://blog.docker.com/2017/11/multi-arch-all-the-things/\">image manifest</a>.\nThe <a href=\"https://blog.docker.com/2014/06/announcing-docker-hub-and-official-repositories/\">DockerHub official images</a> all support\nmultiple architectures.\nThis excellent <a href=\"https://lobradov.github.io/Building-docker-multiarch-images/\">blog post</a> shows how to build your own multi-arch\nimages. The <a href=\"./dns-updater/docker\">dns-updater image</a> also shows this approach.</p>\n<h2 id=\"open-issues\"><a href=\"#open-issues\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Open Issues</h2>\n<p>I've manually built several ARM images which are included in the Ansible playbooks,\nbut ideally the project maintainers would publish official ARM images.\nHere's a list of issues tracking official ARM images:</p>\n<ul>\n<li>Ark: <a href=\"https://github.com/heptio/ark/issues/720\">https://github.com/heptio/ark/issues/720</a> &#x26; <a href=\"https://github.com/heptio/ark/issues/638\">https://github.com/heptio/ark/issues/638</a></li>\n<li>Heketi: <a href=\"https://github.com/heketi/heketi/issues/1470\">https://github.com/heketi/heketi/issues/1470</a></li>\n<li>Gluster: <a href=\"https://github.com/gluster/gluster-containers/issues/112\">https://github.com/gluster/gluster-containers/issues/112</a></li>\n<li>Helm: <a href=\"https://github.com/helm/helm/issues/3269\">https://github.com/helm/helm/issues/3269</a></li>\n<li>nginx-ingress: <a href=\"https://github.com/kubernetes/ingress-nginx/issues/3545\">https://github.com/kubernetes/ingress-nginx/issues/3545</a></li>\n<li>cert-manager: <a href=\"https://github.com/jetstack/cert-manager/issues/608\">https://github.com/jetstack/cert-manager/issues/608</a></li>\n<li>openvpn: hasn't been updated in 2 years</li>\n</ul>\n<h2 id=\"future-work\"><a href=\"#future-work\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Future work</h2>\n<p>Here's a few ideas that would be nice to implement going forward:</p>\n<ul>\n<li>\n<p>Switch to <a href=\"https://github.com/kubernetes-sigs/kubespray\">Kubespray</a> for base Ansible playbooks</p>\n<ul>\n<li>Like this project, Kubespray is a collection of Ansible roles to setup a k8s cluster, but doesn't support ARM out-of-the-box</li>\n<li>I tried for ~1 day to get Kubespray working but I got stuck on GlusterFS volumes hanging on mount</li>\n</ul>\n</li>\n<li>\n<p>Support multi-master deployment of master node and etcd</p>\n<ul>\n<li>The above setup only has a single master k8s node</li>\n<li>If this node goes down you'll be unable to modify any cluster resources, luckily the workers can continue to serve application traffic</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"finished\"><a href=\"#finished\" aria-hidden=\"true\" class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Finished!</h2>\n<p>Please open an issue if you run into trouble.\nThere are a lot of moving pieces here and it probably won't work on the first try.\nAs frustrating as it can be to get this stuff working, it is equally delightful to use once you get it setup.\nPRs to docs and playbooks gladly accepted.</p>\n<p>Bon Voyage!</p>\n<img src=\"https://github.com/ashleymcnamara/gophers/raw/master/KUBERNETES_GOPHER.png\" alt=\"k8s gophers\" title=\"k8s gophers\" width=\"100%\">\n<p>(Gopher image by <a href=\"https://github.com/ashleymcnamara/gophers\">Ashley McNamara</a>)</p>","frontmatter":{"date":"January 06, 2019","path":"/blog/k8s-on-pi","title":"A \"production-ish\" Kubernetes cluster on Raspberry Pi"}}},"pageContext":{}}